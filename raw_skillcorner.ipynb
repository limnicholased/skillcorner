{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0939b0ae-b04f-4a9c-8a24-fb831fac587d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os, uuid, pathlib, shutil\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41a414a-2b7a-4a7e-9969-bcd19ce118c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env = dbutils.widgets.get(\"env\").strip()\n",
    "match_id = dbutils.widgets.get(\"match_id\").strip()\n",
    "if not env:\n",
    "    raise ValueError(\"Widget 'env' is required, e.g. 'dev', staging, prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43944163-6b3a-4b4e-860a-26f256932b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = f\"skillcorner_{env}\"\n",
    "SCHEMA = f\"{CATALOG}.raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5425dd8e-664e-413d-ac62-ab2f98adc060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\")\n",
    "spark.sql(f\"USE {SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efa11b79-e081-4a88-8e2e-5e09dfafd51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generic Spark setting for safe partition overwrites (engine-agnostic)\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd69c11d-a145-46db-bc60-eb8783cd6335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/SkillCorner/opendata/master/data\"\n",
    "matches_index_url = f\"{base_url}/matches.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4bfbc04-c875-4812-940a-268da259ecce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_match_ids() -> List[str]:\n",
    "    data = requests.get(matches_index_url, timeout=60).json()\n",
    "    return [str(x[\"id\"]) for x in data if \"id\" in x]\n",
    "\n",
    "def urls_for_match(mid: str) -> Dict[str, str]:\n",
    "    return {\n",
    "        \"meta\":     f\"{base_url}/matches/{mid}/{mid}_match.json\",\n",
    "        \"tracking\": f\"{base_url}/matches/{mid}/{mid}_tracking_extrapolated.jsonl\",\n",
    "        \"events\":   f\"{base_url}/matches/{mid}/{mid}_dynamic_events.csv\",\n",
    "        \"phases\":   f\"{base_url}/matches/{mid}/{mid}_phases_of_play.csv\",\n",
    "    }\n",
    "\n",
    "def table_exists(ident: str) -> bool:\n",
    "    return spark._jsparkSession.catalog().tableExists(ident)\n",
    "\n",
    "def ensure_table(table_ident: str, schema: T.StructType, partition_by: str = None):\n",
    "    \"\"\"\n",
    "    Create a managed table with the given schema if it doesn't exist.\n",
    "    No storage format is specified (portable).\n",
    "    \"\"\"\n",
    "    if table_exists(table_ident):\n",
    "        return\n",
    "    df_empty = spark.createDataFrame([], schema)\n",
    "    w = df_empty.write.mode(\"error\")\n",
    "    if partition_by:\n",
    "        w = w.partitionBy(partition_by)\n",
    "    w.saveAsTable(table_ident)\n",
    "\n",
    "def overwrite_partition_via_insert(df, table_ident: str, partition_col: str, partition_value: str):\n",
    "    \"\"\"\n",
    "    Portable single-partition overwrite using INSERT OVERWRITE PARTITION.\n",
    "    Avoids engine-specific features; uses only generic Spark SQL.\n",
    "    \"\"\"\n",
    "    non_part_cols = [c for c in df.columns if c != partition_col]\n",
    "    tmp_view = f\"_tmp_{partition_col}_{uuid.uuid4().hex}\"\n",
    "\n",
    "    df.filter(F.col(partition_col) == partition_value).select(*non_part_cols).createOrReplaceTempView(tmp_view)\n",
    "    cols_csv = \", \".join(non_part_cols)\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT OVERWRITE TABLE {table_ident}\n",
    "        PARTITION ({partition_col}='{partition_value}')\n",
    "        SELECT {cols_csv} FROM {tmp_view}\n",
    "    \"\"\")\n",
    "    spark.catalog.dropTempView(tmp_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e786135e-2219-4ca3-85c4-829460501296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure RAW tables (portable)\n",
    "\n",
    "# 1. Catalog of matches (from matches.json) - one row per match entry (json string kept verbatim)\n",
    "ensure_table(\n",
    "    f\"{SCHEMA}.raw_matches_catalog\",\n",
    "    T.StructType([\n",
    "        T.StructField(\"match_id\",   T.StringType(), True),\n",
    "        T.StructField(\"json\",       T.StringType(), True),\n",
    "        T.StructField(\"source_url\", T.StringType(), True),\n",
    "        T.StructField(\"ingested_at\",T.TimestampType(), True),\n",
    "    ]),\n",
    "    partition_by=\"match_id\"\n",
    ")\n",
    "\n",
    "# 2. Per-match match.json (verbatim blob string)\n",
    "ensure_table(\n",
    "    f\"{SCHEMA}.raw_match_json\",\n",
    "    T.StructType([\n",
    "        T.StructField(\"match_id\",   T.StringType(), True),\n",
    "        T.StructField(\"json\",       T.StringType(), True),\n",
    "        T.StructField(\"source_url\", T.StringType(), True),\n",
    "        T.StructField(\"ingested_at\",T.TimestampType(), True),\n",
    "    ]),\n",
    "    partition_by=\"match_id\"\n",
    ")\n",
    "\n",
    "# 3. Per-match tracking JSONL (one JSON line per row)\n",
    "ensure_table(\n",
    "    f\"{SCHEMA}.raw_tracking_jsonl\",\n",
    "    T.StructType([\n",
    "        T.StructField(\"match_id\",    T.StringType(), True),\n",
    "        T.StructField(\"line_number\", T.LongType(),   True),\n",
    "        T.StructField(\"json_line\",   T.StringType(), True),\n",
    "        T.StructField(\"source_url\",  T.StringType(), True),\n",
    "        T.StructField(\"ingested_at\", T.TimestampType(), True),\n",
    "    ]),\n",
    "    partition_by=\"match_id\"\n",
    ")\n",
    "\n",
    "# 4. Per-match events CSV (keep columns as STRING in RAW)\n",
    "ensure_table(\n",
    "    f\"{SCHEMA}.raw_events_csv\",\n",
    "    T.StructType([\n",
    "        T.StructField(\"match_id\",   T.StringType(), True),\n",
    "        # CSV columns will be supplied on write (all cast to STRING)\n",
    "        T.StructField(\"source_url\", T.StringType(), True),\n",
    "        T.StructField(\"ingested_at\",T.TimestampType(), True),\n",
    "    ]),\n",
    "    partition_by=\"match_id\"\n",
    ")\n",
    "\n",
    "# 5. Per-match phases CSV (keep columns as STRING in RAW)\n",
    "ensure_table(\n",
    "    f\"{SCHEMA}.raw_phases_csv\",\n",
    "    T.StructType([\n",
    "        T.StructField(\"match_id\",   T.StringType(), True),\n",
    "        T.StructField(\"source_url\", T.StringType(), True),\n",
    "        T.StructField(\"ingested_at\",T.TimestampType(), True),\n",
    "    ]),\n",
    "    partition_by=\"match_id\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6736fb3-db07-4a36-9132-1b99adaafe15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tiny downloader (to /tmp)\n",
    "\n",
    "TMP_DIR = \"/tmp/skillcorner_raw\"\n",
    "pathlib.Path(TMP_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _tmp(name: str) -> str:\n",
    "    return os.path.join(TMP_DIR, f\"{uuid.uuid4().hex}_{name}\")\n",
    "\n",
    "def download(url: str, to_path: str) -> str:\n",
    "    r = requests.get(url, stream=True, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "    return to_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb421126-4fc0-409a-b667-ef13ce53465d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ingest: matches catalog (all)\n",
    "\n",
    "matches = requests.get(matches_index_url, timeout=60).json()\n",
    "now = datetime.utcnow()\n",
    "\n",
    "rows = []\n",
    "for obj in matches:\n",
    "    mid = str(obj.get(\"id\", \"\"))\n",
    "    if mid:\n",
    "        rows.append((mid, json.dumps(obj, ensure_ascii=False), matches_index_url, now))\n",
    "\n",
    "df_catalog = spark.createDataFrame(rows, schema=\"match_id string, json string, source_url string, ingested_at timestamp\")\n",
    "\n",
    "# Overwrite each partition (safe to re-run)\n",
    "for mid in {r[0] for r in rows}:\n",
    "    overwrite_partition_via_insert(\n",
    "        df_catalog.withColumn(\"match_id\", F.lit(mid)),\n",
    "        f\"{SCHEMA}.raw_matches_catalog\",\n",
    "        partition_col=\"match_id\",\n",
    "        partition_value=mid\n",
    "    )\n",
    "\n",
    "# Ingest: per-match artifacts\n",
    "\n",
    "target_ids = [match_id] if match_id else [str(x[\"id\"]) for x in matches if \"id\" in x]\n",
    "\n",
    "summary = []\n",
    "for mid in target_ids:\n",
    "    urls = urls_for_match(mid)\n",
    "    now_ts = F.current_timestamp()\n",
    "\n",
    "    # 2) match.json -> single row\n",
    "    meta_path = _tmp(\"match.json\"); download(urls[\"meta\"], meta_path)\n",
    "    meta_text = open(meta_path, \"r\", encoding=\"utf-8\").read()\n",
    "    df_meta = (spark.createDataFrame([(meta_text, urls[\"meta\"])], \"json string, source_url string\")\n",
    "               .withColumn(\"ingested_at\", now_ts)\n",
    "               .withColumn(\"match_id\", F.lit(mid))\n",
    "               .select(\"json\",\"source_url\",\"ingested_at\",\"match_id\"))\n",
    "    overwrite_partition_via_insert(\n",
    "        df=df_meta,\n",
    "        table_ident=f\"{SCHEMA}.raw_match_json\",\n",
    "        partition_col=\"match_id\",\n",
    "        partition_value=mid\n",
    "    )\n",
    "\n",
    "    # 3) tracking JSONL -> (line_number, json_line)\n",
    "    trk_path = _tmp(\"tracking.jsonl\"); download(urls[\"tracking\"], trk_path)\n",
    "    rdd = (spark.sparkContext.textFile(f\"file:{trk_path}\")\n",
    "           .zipWithIndex()\n",
    "           .map(lambda x: (x[1] + 1, x[0])))\n",
    "    df_trk = (spark.createDataFrame(rdd, \"line_number long, json_line string\")\n",
    "              .withColumn(\"source_url\", F.lit(urls[\"tracking\"]))\n",
    "              .withColumn(\"ingested_at\", now_ts)\n",
    "              .withColumn(\"match_id\", F.lit(mid))\n",
    "              .select(\"line_number\",\"json_line\",\"source_url\",\"ingested_at\",\"match_id\"))\n",
    "    overwrite_partition_via_insert(\n",
    "        df=df_trk,\n",
    "        table_ident=f\"{SCHEMA}.raw_tracking_jsonl\",\n",
    "        partition_col=\"match_id\",\n",
    "        partition_value=mid\n",
    "    )\n",
    "\n",
    "    # 4) events CSV -> all columns as STRING\n",
    "    evt_path = _tmp(\"events.csv\"); download(urls[\"events\"], evt_path)\n",
    "    df_evt = spark.read.csv(f\"file:{evt_path}\", header=True, inferSchema=False)\n",
    "    # Cast everything to STRING for RAW\n",
    "    df_evt = df_evt.select([F.col(c).cast(\"string\").alias(c) for c in df_evt.columns]) \\\n",
    "                   .withColumn(\"source_url\", F.lit(urls[\"events\"])) \\\n",
    "                   .withColumn(\"ingested_at\", now_ts) \\\n",
    "                   .withColumn(\"match_id\", F.lit(mid))\n",
    "    # Reorder to put non-partition columns first (optional)\n",
    "    non_part_cols = [c for c in df_evt.columns if c != \"match_id\"]\n",
    "    df_evt = df_evt.select(*non_part_cols, \"match_id\")\n",
    "    overwrite_partition_via_insert(\n",
    "        df=df_evt,\n",
    "        table_ident=f\"{SCHEMA}.raw_events_csv\",\n",
    "        partition_col=\"match_id\",\n",
    "        partition_value=mid\n",
    "    )\n",
    "\n",
    "    # 5) phases CSV -> all columns as STRING\n",
    "    pha_path = _tmp(\"phases.csv\"); download(urls[\"phases\"], pha_path)\n",
    "    df_pha = spark.read.csv(f\"file:{pha_path}\", header=True, inferSchema=False)\n",
    "    df_pha = df_pha.select([F.col(c).cast(\"string\").alias(c) for c in df_pha.columns]) \\\n",
    "                   .withColumn(\"source_url\", F.lit(urls[\"phases\"])) \\\n",
    "                   .withColumn(\"ingested_at\", now_ts) \\\n",
    "                   .withColumn(\"match_id\", F.lit(mid))\n",
    "    non_part_cols_p = [c for c in df_pha.columns if c != \"match_id\"]\n",
    "    df_pha = df_pha.select(*non_part_cols_p, \"match_id\")\n",
    "    overwrite_partition_via_insert(\n",
    "        df=df_pha,\n",
    "        table_ident=f\"{SCHEMA}.raw_phases_csv\",\n",
    "        partition_col=\"match_id\",\n",
    "        partition_value=mid\n",
    "    )\n",
    "\n",
    "    summary.append((mid, df_trk.count(), df_evt.count(), df_pha.count()))\n",
    "\n",
    "spark.createDataFrame(summary, schema=\"match_id string, tracking_lines long, events_rows long, phases_rows long\").show(truncate=False)\n",
    "print(f\"Loaded {len(summary)} match(es) into {SCHEMA}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_skillcorner",
   "widgets": {
    "env": {
     "currentValue": "prod",
     "nuid": "9bf1b3e2-c881-443a-af69-29cb25301075",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "match_id": {
     "currentValue": "2017461",
     "nuid": "dcab93af-3981-4a7d-8099-23fbf82f34de",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "match_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "match_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
